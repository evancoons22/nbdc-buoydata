{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96c46e74-60ff-473c-a143-bf73aedd5933",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import functions\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch_geometric.data import  Batch\n",
    "from torch_geometric.loader import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9079e85c-4994-42b4-bb18-1e715c6848de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>buoy_id</th>\n",
       "      <th>WVHT</th>\n",
       "      <th>MWD</th>\n",
       "      <th>APD</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>MWD_sin</th>\n",
       "      <th>MWD_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-02-15 23:00:00</td>\n",
       "      <td>46001</td>\n",
       "      <td>4.8</td>\n",
       "      <td>138.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>56.300</td>\n",
       "      <td>-148.018</td>\n",
       "      <td>0.669131</td>\n",
       "      <td>-0.743145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-02-15 22:00:00</td>\n",
       "      <td>46001</td>\n",
       "      <td>4.7</td>\n",
       "      <td>148.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>56.300</td>\n",
       "      <td>-148.018</td>\n",
       "      <td>0.529919</td>\n",
       "      <td>-0.848048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-02-15 22:00:00</td>\n",
       "      <td>46001</td>\n",
       "      <td>4.9</td>\n",
       "      <td>137.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>56.300</td>\n",
       "      <td>-148.018</td>\n",
       "      <td>0.681998</td>\n",
       "      <td>-0.731354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-02-15 21:00:00</td>\n",
       "      <td>46001</td>\n",
       "      <td>4.5</td>\n",
       "      <td>142.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>56.300</td>\n",
       "      <td>-148.018</td>\n",
       "      <td>0.615661</td>\n",
       "      <td>-0.788011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-02-15 21:00:00</td>\n",
       "      <td>46001</td>\n",
       "      <td>4.6</td>\n",
       "      <td>146.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>56.300</td>\n",
       "      <td>-148.018</td>\n",
       "      <td>0.559193</td>\n",
       "      <td>-0.829038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139274</th>\n",
       "      <td>2024-01-01 02:00:00</td>\n",
       "      <td>51212</td>\n",
       "      <td>1.4</td>\n",
       "      <td>314.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>21.323</td>\n",
       "      <td>-158.149</td>\n",
       "      <td>-0.719340</td>\n",
       "      <td>0.694658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139275</th>\n",
       "      <td>2024-01-01 01:00:00</td>\n",
       "      <td>51212</td>\n",
       "      <td>1.5</td>\n",
       "      <td>316.0</td>\n",
       "      <td>9.1</td>\n",
       "      <td>21.323</td>\n",
       "      <td>-158.149</td>\n",
       "      <td>-0.694658</td>\n",
       "      <td>0.719340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139276</th>\n",
       "      <td>2024-01-01 01:00:00</td>\n",
       "      <td>51212</td>\n",
       "      <td>1.5</td>\n",
       "      <td>312.0</td>\n",
       "      <td>9.8</td>\n",
       "      <td>21.323</td>\n",
       "      <td>-158.149</td>\n",
       "      <td>-0.743145</td>\n",
       "      <td>0.669131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139277</th>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "      <td>51212</td>\n",
       "      <td>1.4</td>\n",
       "      <td>315.0</td>\n",
       "      <td>9.8</td>\n",
       "      <td>21.323</td>\n",
       "      <td>-158.149</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139278</th>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "      <td>51212</td>\n",
       "      <td>1.4</td>\n",
       "      <td>314.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>21.323</td>\n",
       "      <td>-158.149</td>\n",
       "      <td>-0.719340</td>\n",
       "      <td>0.694658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134924 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  datetime buoy_id  WVHT    MWD   APD     lat     long  \\\n",
       "0      2024-02-15 23:00:00   46001   4.8  138.0   7.7  56.300 -148.018   \n",
       "1      2024-02-15 22:00:00   46001   4.7  148.0   7.4  56.300 -148.018   \n",
       "2      2024-02-15 22:00:00   46001   4.9  137.0   7.8  56.300 -148.018   \n",
       "3      2024-02-15 21:00:00   46001   4.5  142.0   7.7  56.300 -148.018   \n",
       "4      2024-02-15 21:00:00   46001   4.6  146.0   7.9  56.300 -148.018   \n",
       "...                    ...     ...   ...    ...   ...     ...      ...   \n",
       "139274 2024-01-01 02:00:00   51212   1.4  314.0   8.0  21.323 -158.149   \n",
       "139275 2024-01-01 01:00:00   51212   1.5  316.0   9.1  21.323 -158.149   \n",
       "139276 2024-01-01 01:00:00   51212   1.5  312.0   9.8  21.323 -158.149   \n",
       "139277 2024-01-01 00:00:00   51212   1.4  315.0   9.8  21.323 -158.149   \n",
       "139278 2024-01-01 00:00:00   51212   1.4  314.0  10.3  21.323 -158.149   \n",
       "\n",
       "         MWD_sin   MWD_cos  \n",
       "0       0.669131 -0.743145  \n",
       "1       0.529919 -0.848048  \n",
       "2       0.681998 -0.731354  \n",
       "3       0.615661 -0.788011  \n",
       "4       0.559193 -0.829038  \n",
       "...          ...       ...  \n",
       "139274 -0.719340  0.694658  \n",
       "139275 -0.694658  0.719340  \n",
       "139276 -0.743145  0.669131  \n",
       "139277 -0.707107  0.707107  \n",
       "139278 -0.719340  0.694658  \n",
       "\n",
       "[134924 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect('db.db')\n",
    "data = pd.read_sql_query(\"SELECT * from test\", conn)\n",
    "data = functions.cleanData(data)\n",
    "data['MWD_sin'] = np.sin(data['MWD'] * (np.pi/180))\n",
    "data['MWD_cos'] = np.cos(data['MWD'] * (np.pi/180))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76de7c8e-7ffa-4231-8e89-a61f2a129ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodde MWD (degrees) into sin and cos direction\n",
    "data['MWD_sin'] = np.sin(data['MWD'] * (np.pi/180))\n",
    "data['MWD_cos'] = np.cos(data['MWD'] * (np.pi/180))\n",
    "data['is_present'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a92c9ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'buoy_id', 'WVHT', 'MWD', 'APD', 'lat', 'long', 'MWD_sin',\n",
       "       'MWD_cos', 'is_present'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "866b76a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_buoy_ids = data['buoy_id'].unique()\n",
    "# Get all unique times\n",
    "all_times = data['datetime'].unique()\n",
    "\n",
    "# Iterate through each time\n",
    "for time in all_times:\n",
    "    # Get the data for the current time\n",
    "    time_data = data[data['datetime'] == time]\n",
    "    \n",
    "    # Get the missing buoy_ids for the current time\n",
    "    missing_buoy_ids = np.setdiff1d(all_buoy_ids, time_data['buoy_id'])\n",
    "    \n",
    "    # if len(missing_buoy_ids) != 0: \n",
    "        # print(\"there is\", len(missing_buoy_ids), \" missing buoys at time: \", time, \"they are: \", missing_buoy_ids)  \n",
    "\n",
    "    # Iterate through each missing buoy_id\n",
    "    for buoy_id in missing_buoy_ids:\n",
    "        # Find the previous time\n",
    "        # previous_time = data[data['buoy_id'] == buoy_id]['datetime'].max()\n",
    "        \n",
    "        # Get the previous row for the missing buoy_id\n",
    "        # previous_row = data[(data['datetime'] == previous_time) & (data['buoy_id'] == buoy_id)]\n",
    "        \n",
    "        # Create a new row with the previous values and the current time\n",
    "        # new_row = previous_row.copy()\n",
    "        # new_row['datetime'] = time\n",
    "        # Create a new row with the missing buoy_id, current time, and is_present = 0\n",
    "\n",
    "        new_row = pd.DataFrame({'buoy_id': [buoy_id], 'datetime': [time], 'is_present': [0]})\n",
    "    \n",
    "        # Append the new row to the data\n",
    "        data = pd.concat([data, new_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2101a989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example DataFrame\n",
    "# df = pd.DataFrame(your_data)\n",
    "\n",
    "# Normalize the continuous features\n",
    "scaler = StandardScaler()\n",
    "data[['WVHT', 'APD', 'MWD_sin', 'MWD_cos']] = scaler.fit_transform(data[['WVHT', 'APD', 'MWD_sin', 'MWD_cos']])\n",
    "\n",
    "# Add binary feature for missing data\n",
    "# Here, you would need a way to identify missing data. Assuming 'WVHT' is null if data is missing\n",
    "# data['data_present'] = data['WVHT'].notnull().astype(int)\n",
    "\n",
    "# Assuming missing data is filled with zeros or imputed before this step\n",
    "data.fillna(0, inplace=True)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "features = torch.tensor(data[['WVHT', 'APD', 'MWD_sin', 'MWD_cos', 'lat', 'long', 'is_present']].values, dtype=torch.float)\n",
    "times = data['datetime'].unique()\n",
    "\n",
    "graphs = []\n",
    "for time in times:\n",
    "    time_data = data[data['datetime'] == time]\n",
    "    node_features = torch.tensor(time_data[['WVHT', 'APD', 'MWD_sin', 'MWD_cos', 'lat', 'long', 'is_present']].values, dtype=torch.float)\n",
    "    \n",
    "    # Fully connected graph: all nodes are connected, excluding self-loops\n",
    "    num_nodes = len(time_data)\n",
    "    edge_index = [[i, j] for i in range(num_nodes) for j in range(num_nodes) if i != j]\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    # Create graph data object\n",
    "    graph_data = Data(x=node_features, edge_index=edge_index)\n",
    "    graphs.append(graph_data)\n",
    "\n",
    "# Now, `graphs` contains a list of Data objects for each time step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d4ac455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1104"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fbc3d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphEmbeddingModel(torch.nn.Module):\n",
    "    def __init__(self, num_node_features):\n",
    "        super(GraphEmbeddingModel, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 128)\n",
    "        self.conv2 = GCNConv(128, 64)\n",
    "        self.fc = Linear(64, 32)  # Embedding size\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "\n",
    "        # Global mean pooling to get a graph-level embedding\n",
    "        x = global_mean_pool(x, torch.zeros(x.size(0), dtype=int).to(x.device))  # Assuming batch size of 1 for simplicity\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SequenceModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, sequence_length, nhead, num_encoder_layers, dim_feedforward):\n",
    "        super(SequenceModel, self).__init__()\n",
    "        self.encoder_layer = TransformerEncoderLayer(d_model=input_size, nhead=nhead, dim_feedforward=dim_feedforward)\n",
    "        self.transformer_encoder = TransformerEncoder(self.encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.fc_out = Linear(input_size, input_size)  # Predicting the next graph embedding\n",
    "\n",
    "    def forward(self, src):\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = self.fc_out(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ea8a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming graphs is your list of Data objects\n",
    "train_size = int(len(graphs) * 0.8)\n",
    "train_graphs, test_graphs = graphs[:train_size], graphs[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "492b641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_sequences(embeddings, sequence_length):\n",
    "    \"\"\"\n",
    "    Create sequences of embeddings for training the sequence model.\n",
    "    \n",
    "    :param embeddings: A list of embeddings, where each embedding corresponds to a graph.\n",
    "    :param sequence_length: The fixed length of each sequence.\n",
    "    :return: A list of sequences suitable for training/testing the sequence model.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    for i in range(len(embeddings) - sequence_length):\n",
    "        sequence = torch.stack(embeddings[i:i+sequence_length])\n",
    "        sequences.append(torch.Tensor(sequence))\n",
    "    return torch.stack(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "907c526e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "883\n",
      "883\n"
     ]
    }
   ],
   "source": [
    "# Assume graph_model is your GraphEmbeddingModel instance\n",
    "def graph_to_embedding(data_loader, graph_model, device):\n",
    "    \"\"\"\n",
    "    Process graphs through the GNN model to obtain embeddings.\n",
    "    \n",
    "    :param data_loader: DataLoader containing graph Data objects.\n",
    "    :param graph_model: The trained graph embedding model.\n",
    "    :param device: The device (CPU or GPU) to perform computation on.\n",
    "    :return: A list of graph embeddings.\n",
    "    \"\"\"\n",
    "    graph_model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            data = data.to(device)\n",
    "            embedding = graph_model(data)\n",
    "            embeddings.extend(embedding.detach().cpu())\n",
    "    return embeddings\n",
    "\n",
    "# Create a DataLoader for your graphs\n",
    "batch_size = 32 # Adjust based on your system's capability\n",
    "graph_model = GraphEmbeddingModel(num_node_features=7)  # Assuming 7 features for each node\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_embeddings = graph_to_embedding(train_graphs, graph_model, device)\n",
    "\n",
    "# train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\n",
    "print(len(train_graphs))\n",
    "\n",
    "# Convert graphs to embeddings\n",
    "# graph_model = GraphEmbeddingModel(num_node_features=7)  # Assuming 7 features for each node\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# train_embeddings = graph_to_embedding(train_loader, graph_model, device)\n",
    "print(len(train_embeddings))\n",
    "\n",
    "# Now, use create_embedding_sequences to prepare input for the Transformer\n",
    "sequence_length = 96  # Define the sequence length for your application\n",
    "sequences = create_embedding_sequences(train_embeddings, sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d85a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Split sequences into inputs and targets\n",
    "input_sequences = sequences[:-1]  # All but the last for each sequence\n",
    "target_sequences = sequences[1:]  # From the second to the end for each sequence\n",
    "\n",
    "# Create TensorDataset and DataLoader for sequence data\n",
    "sequence_dataset = TensorDataset(input_sequences, target_sequences)\n",
    "sequence_loader = DataLoader(sequence_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Now, you can iterate over sequence_loader to train your Transformer model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a4a3e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evancoons/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "graph_model = GraphEmbeddingModel(num_node_features=7).to(device)  # 7 node features as per your dataset\n",
    "sequence_model = SequenceModel(input_size=32, sequence_length=10, nhead=4, num_encoder_layers=3, dim_feedforward=128).to(device)  # Adjust parameters as needed\n",
    "\n",
    "optimizer = torch.optim.Adam(list(graph_model.parameters()) + list(sequence_model.parameters()), lr=0.001)\n",
    "criterion = torch.nn.MSELoss()  # Assuming you're predicting continuous values\n",
    "\n",
    "# Example training loop for one epoch\n",
    "graph_model.train()\n",
    "sequence_model.train()\n",
    "sequence_length = 96 # 4 days\n",
    "# for data in train_graphs:  # You need to adapt this loop to handle sequences properly\n",
    "embedding_sequence = create_embedding_sequences(train_embeddings, sequence_length + 1)\n",
    "\n",
    "# for i in range(len(train_graphs) - sequence_length):\n",
    "for i, seq in enumerate(embedding_sequence[:-1]):\n",
    "    # data = data.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    #graph_embeddings = [graph_model(x) for x in train_graphs[i:i+sequence_length]]\n",
    "    # graph_embeddings = torch.stack(graph_embeddings)  \n",
    "\n",
    "    graph_sequence = embedding_sequence[i]\n",
    "    target_sequence = embedding_sequence[i+1]\n",
    "\n",
    "    # sequence_of_embeddings = create_embedding_sequences(graph_embeddings, sequence_length)\n",
    "    # target_embeddings = sequence_of_embeddings[1:]  # Assuming you're predicting the next embedding in the sequence\n",
    "\n",
    "    # Here, you should collect a sequence of embeddings and then pass them through the sequence model\n",
    "    # output = sequence_model(sequence_of_embeddings)\n",
    "    output = sequence_model(graph_sequence)\n",
    "    loss = criterion(output, target_sequence)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Similarly, set up your evaluation loop, ensuring to track and evaluate the loss over time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99626447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/25], Loss: 0.0007\n",
      "Epoch [1/10], Step [2/25], Loss: 0.0006\n",
      "Epoch [1/10], Step [3/25], Loss: 0.0006\n",
      "Epoch [1/10], Step [4/25], Loss: 0.0006\n",
      "Epoch [1/10], Step [5/25], Loss: 0.0006\n",
      "Epoch [1/10], Step [6/25], Loss: 0.0006\n",
      "Epoch [1/10], Step [7/25], Loss: 0.0006\n",
      "Epoch [1/10], Step [8/25], Loss: 0.0006\n",
      "Epoch [1/10], Step [9/25], Loss: 0.0006\n",
      "Epoch [1/10], Step [10/25], Loss: 0.0006\n",
      "Epoch [1/10], Step [11/25], Loss: 0.0006\n",
      "Epoch [1/10], Step [12/25], Loss: 0.0005\n",
      "Epoch [1/10], Step [13/25], Loss: 0.0006\n",
      "Epoch [1/10], Step [14/25], Loss: 0.0005\n",
      "Epoch [1/10], Step [15/25], Loss: 0.0005\n",
      "Epoch [1/10], Step [16/25], Loss: 0.0006\n",
      "Epoch [1/10], Step [17/25], Loss: 0.0005\n",
      "Epoch [1/10], Step [18/25], Loss: 0.0005\n",
      "Epoch [1/10], Step [19/25], Loss: 0.0006\n",
      "Epoch [1/10], Step [20/25], Loss: 0.0006\n",
      "Epoch [1/10], Step [21/25], Loss: 0.0005\n",
      "Epoch [1/10], Step [22/25], Loss: 0.0005\n",
      "Epoch [1/10], Step [23/25], Loss: 0.0005\n",
      "Epoch [1/10], Step [24/25], Loss: 0.0005\n",
      "Epoch [1/10], Step [25/25], Loss: 0.0005\n",
      "Epoch [2/10], Step [1/25], Loss: 0.0005\n",
      "Epoch [2/10], Step [2/25], Loss: 0.0005\n",
      "Epoch [2/10], Step [3/25], Loss: 0.0005\n",
      "Epoch [2/10], Step [4/25], Loss: 0.0005\n",
      "Epoch [2/10], Step [5/25], Loss: 0.0005\n",
      "Epoch [2/10], Step [6/25], Loss: 0.0005\n",
      "Epoch [2/10], Step [7/25], Loss: 0.0005\n",
      "Epoch [2/10], Step [8/25], Loss: 0.0005\n",
      "Epoch [2/10], Step [9/25], Loss: 0.0005\n",
      "Epoch [2/10], Step [10/25], Loss: 0.0005\n",
      "Epoch [2/10], Step [11/25], Loss: 0.0005\n",
      "Epoch [2/10], Step [12/25], Loss: 0.0005\n",
      "Epoch [2/10], Step [13/25], Loss: 0.0005\n",
      "Epoch [2/10], Step [14/25], Loss: 0.0005\n",
      "Epoch [2/10], Step [15/25], Loss: 0.0005\n",
      "Epoch [2/10], Step [16/25], Loss: 0.0005\n",
      "Epoch [2/10], Step [17/25], Loss: 0.0005\n",
      "Epoch [2/10], Step [18/25], Loss: 0.0005\n",
      "Epoch [2/10], Step [19/25], Loss: 0.0005\n",
      "Epoch [2/10], Step [20/25], Loss: 0.0005\n",
      "Epoch [2/10], Step [21/25], Loss: 0.0004\n",
      "Epoch [2/10], Step [22/25], Loss: 0.0005\n",
      "Epoch [2/10], Step [23/25], Loss: 0.0004\n",
      "Epoch [2/10], Step [24/25], Loss: 0.0004\n",
      "Epoch [2/10], Step [25/25], Loss: 0.0005\n",
      "Epoch [3/10], Step [1/25], Loss: 0.0005\n",
      "Epoch [3/10], Step [2/25], Loss: 0.0005\n",
      "Epoch [3/10], Step [3/25], Loss: 0.0005\n",
      "Epoch [3/10], Step [4/25], Loss: 0.0005\n",
      "Epoch [3/10], Step [5/25], Loss: 0.0005\n",
      "Epoch [3/10], Step [6/25], Loss: 0.0005\n",
      "Epoch [3/10], Step [7/25], Loss: 0.0004\n",
      "Epoch [3/10], Step [8/25], Loss: 0.0005\n",
      "Epoch [3/10], Step [9/25], Loss: 0.0004\n",
      "Epoch [3/10], Step [10/25], Loss: 0.0005\n",
      "Epoch [3/10], Step [11/25], Loss: 0.0005\n",
      "Epoch [3/10], Step [12/25], Loss: 0.0005\n",
      "Epoch [3/10], Step [13/25], Loss: 0.0004\n",
      "Epoch [3/10], Step [14/25], Loss: 0.0004\n",
      "Epoch [3/10], Step [15/25], Loss: 0.0005\n",
      "Epoch [3/10], Step [16/25], Loss: 0.0005\n",
      "Epoch [3/10], Step [17/25], Loss: 0.0004\n",
      "Epoch [3/10], Step [18/25], Loss: 0.0004\n",
      "Epoch [3/10], Step [19/25], Loss: 0.0004\n",
      "Epoch [3/10], Step [20/25], Loss: 0.0004\n",
      "Epoch [3/10], Step [21/25], Loss: 0.0005\n",
      "Epoch [3/10], Step [22/25], Loss: 0.0005\n",
      "Epoch [3/10], Step [23/25], Loss: 0.0004\n",
      "Epoch [3/10], Step [24/25], Loss: 0.0004\n",
      "Epoch [3/10], Step [25/25], Loss: 0.0004\n",
      "Epoch [4/10], Step [1/25], Loss: 0.0004\n",
      "Epoch [4/10], Step [2/25], Loss: 0.0004\n",
      "Epoch [4/10], Step [3/25], Loss: 0.0004\n",
      "Epoch [4/10], Step [4/25], Loss: 0.0004\n",
      "Epoch [4/10], Step [5/25], Loss: 0.0004\n",
      "Epoch [4/10], Step [6/25], Loss: 0.0004\n",
      "Epoch [4/10], Step [7/25], Loss: 0.0005\n",
      "Epoch [4/10], Step [8/25], Loss: 0.0004\n",
      "Epoch [4/10], Step [9/25], Loss: 0.0004\n",
      "Epoch [4/10], Step [10/25], Loss: 0.0004\n",
      "Epoch [4/10], Step [11/25], Loss: 0.0005\n",
      "Epoch [4/10], Step [12/25], Loss: 0.0005\n",
      "Epoch [4/10], Step [13/25], Loss: 0.0004\n",
      "Epoch [4/10], Step [14/25], Loss: 0.0004\n",
      "Epoch [4/10], Step [15/25], Loss: 0.0004\n",
      "Epoch [4/10], Step [16/25], Loss: 0.0004\n",
      "Epoch [4/10], Step [17/25], Loss: 0.0004\n",
      "Epoch [4/10], Step [18/25], Loss: 0.0004\n",
      "Epoch [4/10], Step [19/25], Loss: 0.0004\n",
      "Epoch [4/10], Step [20/25], Loss: 0.0004\n",
      "Epoch [4/10], Step [21/25], Loss: 0.0004\n",
      "Epoch [4/10], Step [22/25], Loss: 0.0004\n",
      "Epoch [4/10], Step [23/25], Loss: 0.0004\n",
      "Epoch [4/10], Step [24/25], Loss: 0.0004\n",
      "Epoch [4/10], Step [25/25], Loss: 0.0004\n",
      "Epoch [5/10], Step [1/25], Loss: 0.0004\n",
      "Epoch [5/10], Step [2/25], Loss: 0.0005\n",
      "Epoch [5/10], Step [3/25], Loss: 0.0004\n",
      "Epoch [5/10], Step [4/25], Loss: 0.0004\n",
      "Epoch [5/10], Step [5/25], Loss: 0.0004\n",
      "Epoch [5/10], Step [6/25], Loss: 0.0004\n",
      "Epoch [5/10], Step [7/25], Loss: 0.0004\n",
      "Epoch [5/10], Step [8/25], Loss: 0.0004\n",
      "Epoch [5/10], Step [9/25], Loss: 0.0004\n",
      "Epoch [5/10], Step [10/25], Loss: 0.0004\n",
      "Epoch [5/10], Step [11/25], Loss: 0.0004\n",
      "Epoch [5/10], Step [12/25], Loss: 0.0004\n",
      "Epoch [5/10], Step [13/25], Loss: 0.0004\n",
      "Epoch [5/10], Step [14/25], Loss: 0.0004\n",
      "Epoch [5/10], Step [15/25], Loss: 0.0004\n",
      "Epoch [5/10], Step [16/25], Loss: 0.0004\n",
      "Epoch [5/10], Step [17/25], Loss: 0.0004\n",
      "Epoch [5/10], Step [18/25], Loss: 0.0004\n",
      "Epoch [5/10], Step [19/25], Loss: 0.0004\n",
      "Epoch [5/10], Step [20/25], Loss: 0.0005\n",
      "Epoch [5/10], Step [21/25], Loss: 0.0004\n",
      "Epoch [5/10], Step [22/25], Loss: 0.0004\n",
      "Epoch [5/10], Step [23/25], Loss: 0.0004\n",
      "Epoch [5/10], Step [24/25], Loss: 0.0004\n",
      "Epoch [5/10], Step [25/25], Loss: 0.0004\n",
      "Epoch [6/10], Step [1/25], Loss: 0.0004\n",
      "Epoch [6/10], Step [2/25], Loss: 0.0004\n",
      "Epoch [6/10], Step [3/25], Loss: 0.0004\n",
      "Epoch [6/10], Step [4/25], Loss: 0.0004\n",
      "Epoch [6/10], Step [5/25], Loss: 0.0004\n",
      "Epoch [6/10], Step [6/25], Loss: 0.0004\n",
      "Epoch [6/10], Step [7/25], Loss: 0.0004\n",
      "Epoch [6/10], Step [8/25], Loss: 0.0004\n",
      "Epoch [6/10], Step [9/25], Loss: 0.0004\n",
      "Epoch [6/10], Step [10/25], Loss: 0.0004\n",
      "Epoch [6/10], Step [11/25], Loss: 0.0004\n",
      "Epoch [6/10], Step [12/25], Loss: 0.0004\n",
      "Epoch [6/10], Step [13/25], Loss: 0.0004\n",
      "Epoch [6/10], Step [14/25], Loss: 0.0004\n",
      "Epoch [6/10], Step [15/25], Loss: 0.0004\n",
      "Epoch [6/10], Step [16/25], Loss: 0.0004\n",
      "Epoch [6/10], Step [17/25], Loss: 0.0004\n",
      "Epoch [6/10], Step [18/25], Loss: 0.0004\n",
      "Epoch [6/10], Step [19/25], Loss: 0.0004\n",
      "Epoch [6/10], Step [20/25], Loss: 0.0004\n",
      "Epoch [6/10], Step [21/25], Loss: 0.0004\n",
      "Epoch [6/10], Step [22/25], Loss: 0.0004\n",
      "Epoch [6/10], Step [23/25], Loss: 0.0004\n",
      "Epoch [6/10], Step [24/25], Loss: 0.0004\n",
      "Epoch [6/10], Step [25/25], Loss: 0.0004\n",
      "Epoch [7/10], Step [1/25], Loss: 0.0004\n",
      "Epoch [7/10], Step [2/25], Loss: 0.0004\n",
      "Epoch [7/10], Step [3/25], Loss: 0.0004\n",
      "Epoch [7/10], Step [4/25], Loss: 0.0004\n",
      "Epoch [7/10], Step [5/25], Loss: 0.0004\n",
      "Epoch [7/10], Step [6/25], Loss: 0.0004\n",
      "Epoch [7/10], Step [7/25], Loss: 0.0004\n",
      "Epoch [7/10], Step [8/25], Loss: 0.0004\n",
      "Epoch [7/10], Step [9/25], Loss: 0.0004\n",
      "Epoch [7/10], Step [10/25], Loss: 0.0004\n",
      "Epoch [7/10], Step [11/25], Loss: 0.0004\n",
      "Epoch [7/10], Step [12/25], Loss: 0.0004\n",
      "Epoch [7/10], Step [13/25], Loss: 0.0004\n",
      "Epoch [7/10], Step [14/25], Loss: 0.0004\n",
      "Epoch [7/10], Step [15/25], Loss: 0.0004\n",
      "Epoch [7/10], Step [16/25], Loss: 0.0004\n",
      "Epoch [7/10], Step [17/25], Loss: 0.0004\n",
      "Epoch [7/10], Step [18/25], Loss: 0.0004\n",
      "Epoch [7/10], Step [19/25], Loss: 0.0004\n",
      "Epoch [7/10], Step [20/25], Loss: 0.0004\n",
      "Epoch [7/10], Step [21/25], Loss: 0.0004\n",
      "Epoch [7/10], Step [22/25], Loss: 0.0004\n",
      "Epoch [7/10], Step [23/25], Loss: 0.0004\n",
      "Epoch [7/10], Step [24/25], Loss: 0.0004\n",
      "Epoch [7/10], Step [25/25], Loss: 0.0004\n",
      "Epoch [8/10], Step [1/25], Loss: 0.0004\n",
      "Epoch [8/10], Step [2/25], Loss: 0.0004\n",
      "Epoch [8/10], Step [3/25], Loss: 0.0004\n",
      "Epoch [8/10], Step [4/25], Loss: 0.0004\n",
      "Epoch [8/10], Step [5/25], Loss: 0.0004\n",
      "Epoch [8/10], Step [6/25], Loss: 0.0004\n",
      "Epoch [8/10], Step [7/25], Loss: 0.0004\n",
      "Epoch [8/10], Step [8/25], Loss: 0.0004\n",
      "Epoch [8/10], Step [9/25], Loss: 0.0003\n",
      "Epoch [8/10], Step [10/25], Loss: 0.0004\n",
      "Epoch [8/10], Step [11/25], Loss: 0.0003\n",
      "Epoch [8/10], Step [12/25], Loss: 0.0004\n",
      "Epoch [8/10], Step [13/25], Loss: 0.0004\n",
      "Epoch [8/10], Step [14/25], Loss: 0.0004\n",
      "Epoch [8/10], Step [15/25], Loss: 0.0003\n",
      "Epoch [8/10], Step [16/25], Loss: 0.0004\n",
      "Epoch [8/10], Step [17/25], Loss: 0.0004\n",
      "Epoch [8/10], Step [18/25], Loss: 0.0003\n",
      "Epoch [8/10], Step [19/25], Loss: 0.0003\n",
      "Epoch [8/10], Step [20/25], Loss: 0.0003\n",
      "Epoch [8/10], Step [21/25], Loss: 0.0004\n",
      "Epoch [8/10], Step [22/25], Loss: 0.0003\n",
      "Epoch [8/10], Step [23/25], Loss: 0.0003\n",
      "Epoch [8/10], Step [24/25], Loss: 0.0003\n",
      "Epoch [8/10], Step [25/25], Loss: 0.0004\n",
      "Epoch [9/10], Step [1/25], Loss: 0.0003\n",
      "Epoch [9/10], Step [2/25], Loss: 0.0004\n",
      "Epoch [9/10], Step [3/25], Loss: 0.0004\n",
      "Epoch [9/10], Step [4/25], Loss: 0.0004\n",
      "Epoch [9/10], Step [5/25], Loss: 0.0003\n",
      "Epoch [9/10], Step [6/25], Loss: 0.0003\n",
      "Epoch [9/10], Step [7/25], Loss: 0.0003\n",
      "Epoch [9/10], Step [8/25], Loss: 0.0003\n",
      "Epoch [9/10], Step [9/25], Loss: 0.0003\n",
      "Epoch [9/10], Step [10/25], Loss: 0.0004\n",
      "Epoch [9/10], Step [11/25], Loss: 0.0003\n",
      "Epoch [9/10], Step [12/25], Loss: 0.0004\n",
      "Epoch [9/10], Step [13/25], Loss: 0.0003\n",
      "Epoch [9/10], Step [14/25], Loss: 0.0003\n",
      "Epoch [9/10], Step [15/25], Loss: 0.0003\n",
      "Epoch [9/10], Step [16/25], Loss: 0.0004\n",
      "Epoch [9/10], Step [17/25], Loss: 0.0004\n",
      "Epoch [9/10], Step [18/25], Loss: 0.0003\n",
      "Epoch [9/10], Step [19/25], Loss: 0.0003\n",
      "Epoch [9/10], Step [20/25], Loss: 0.0003\n",
      "Epoch [9/10], Step [21/25], Loss: 0.0004\n",
      "Epoch [9/10], Step [22/25], Loss: 0.0003\n",
      "Epoch [9/10], Step [23/25], Loss: 0.0003\n",
      "Epoch [9/10], Step [24/25], Loss: 0.0003\n",
      "Epoch [9/10], Step [25/25], Loss: 0.0003\n",
      "Epoch [10/10], Step [1/25], Loss: 0.0003\n",
      "Epoch [10/10], Step [2/25], Loss: 0.0003\n",
      "Epoch [10/10], Step [3/25], Loss: 0.0004\n",
      "Epoch [10/10], Step [4/25], Loss: 0.0003\n",
      "Epoch [10/10], Step [5/25], Loss: 0.0003\n",
      "Epoch [10/10], Step [6/25], Loss: 0.0003\n",
      "Epoch [10/10], Step [7/25], Loss: 0.0003\n",
      "Epoch [10/10], Step [8/25], Loss: 0.0003\n",
      "Epoch [10/10], Step [9/25], Loss: 0.0003\n",
      "Epoch [10/10], Step [10/25], Loss: 0.0003\n",
      "Epoch [10/10], Step [11/25], Loss: 0.0003\n",
      "Epoch [10/10], Step [12/25], Loss: 0.0003\n",
      "Epoch [10/10], Step [13/25], Loss: 0.0003\n",
      "Epoch [10/10], Step [14/25], Loss: 0.0003\n",
      "Epoch [10/10], Step [15/25], Loss: 0.0003\n",
      "Epoch [10/10], Step [16/25], Loss: 0.0003\n",
      "Epoch [10/10], Step [17/25], Loss: 0.0003\n",
      "Epoch [10/10], Step [18/25], Loss: 0.0003\n",
      "Epoch [10/10], Step [19/25], Loss: 0.0003\n",
      "Epoch [10/10], Step [20/25], Loss: 0.0003\n",
      "Epoch [10/10], Step [21/25], Loss: 0.0003\n",
      "Epoch [10/10], Step [22/25], Loss: 0.0003\n",
      "Epoch [10/10], Step [23/25], Loss: 0.0003\n",
      "Epoch [10/10], Step [24/25], Loss: 0.0003\n",
      "Epoch [10/10], Step [25/25], Loss: 0.0003\n"
     ]
    }
   ],
   "source": [
    "# Now, you can iterate over sequence_loader to train your Transformer model\n",
    "for epoch in range(10):\n",
    "    for i, (inputs, targets) in enumerate(sequence_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = sequence_model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch [{epoch+1}/{10}], Step [{i+1}/{len(sequence_loader)}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1883c78c",
   "metadata": {},
   "source": [
    "## ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f1dd214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>buoy_id</th>\n",
       "      <th>WVHT</th>\n",
       "      <th>MWD</th>\n",
       "      <th>APD</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>MWD_sin</th>\n",
       "      <th>MWD_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-02-15 23:00:00</td>\n",
       "      <td>46001</td>\n",
       "      <td>4.8</td>\n",
       "      <td>138.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>56.300</td>\n",
       "      <td>-148.018</td>\n",
       "      <td>0.669131</td>\n",
       "      <td>-0.743145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-02-15 22:00:00</td>\n",
       "      <td>46001</td>\n",
       "      <td>4.7</td>\n",
       "      <td>148.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>56.300</td>\n",
       "      <td>-148.018</td>\n",
       "      <td>0.529919</td>\n",
       "      <td>-0.848048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-02-15 22:00:00</td>\n",
       "      <td>46001</td>\n",
       "      <td>4.9</td>\n",
       "      <td>137.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>56.300</td>\n",
       "      <td>-148.018</td>\n",
       "      <td>0.681998</td>\n",
       "      <td>-0.731354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-02-15 21:00:00</td>\n",
       "      <td>46001</td>\n",
       "      <td>4.5</td>\n",
       "      <td>142.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>56.300</td>\n",
       "      <td>-148.018</td>\n",
       "      <td>0.615661</td>\n",
       "      <td>-0.788011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-02-15 21:00:00</td>\n",
       "      <td>46001</td>\n",
       "      <td>4.6</td>\n",
       "      <td>146.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>56.300</td>\n",
       "      <td>-148.018</td>\n",
       "      <td>0.559193</td>\n",
       "      <td>-0.829038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139274</th>\n",
       "      <td>2024-01-01 02:00:00</td>\n",
       "      <td>51212</td>\n",
       "      <td>1.4</td>\n",
       "      <td>314.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>21.323</td>\n",
       "      <td>-158.149</td>\n",
       "      <td>-0.719340</td>\n",
       "      <td>0.694658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139275</th>\n",
       "      <td>2024-01-01 01:00:00</td>\n",
       "      <td>51212</td>\n",
       "      <td>1.5</td>\n",
       "      <td>316.0</td>\n",
       "      <td>9.1</td>\n",
       "      <td>21.323</td>\n",
       "      <td>-158.149</td>\n",
       "      <td>-0.694658</td>\n",
       "      <td>0.719340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139276</th>\n",
       "      <td>2024-01-01 01:00:00</td>\n",
       "      <td>51212</td>\n",
       "      <td>1.5</td>\n",
       "      <td>312.0</td>\n",
       "      <td>9.8</td>\n",
       "      <td>21.323</td>\n",
       "      <td>-158.149</td>\n",
       "      <td>-0.743145</td>\n",
       "      <td>0.669131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139277</th>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "      <td>51212</td>\n",
       "      <td>1.4</td>\n",
       "      <td>315.0</td>\n",
       "      <td>9.8</td>\n",
       "      <td>21.323</td>\n",
       "      <td>-158.149</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139278</th>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "      <td>51212</td>\n",
       "      <td>1.4</td>\n",
       "      <td>314.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>21.323</td>\n",
       "      <td>-158.149</td>\n",
       "      <td>-0.719340</td>\n",
       "      <td>0.694658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134924 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  datetime buoy_id  WVHT    MWD   APD     lat     long  \\\n",
       "0      2024-02-15 23:00:00   46001   4.8  138.0   7.7  56.300 -148.018   \n",
       "1      2024-02-15 22:00:00   46001   4.7  148.0   7.4  56.300 -148.018   \n",
       "2      2024-02-15 22:00:00   46001   4.9  137.0   7.8  56.300 -148.018   \n",
       "3      2024-02-15 21:00:00   46001   4.5  142.0   7.7  56.300 -148.018   \n",
       "4      2024-02-15 21:00:00   46001   4.6  146.0   7.9  56.300 -148.018   \n",
       "...                    ...     ...   ...    ...   ...     ...      ...   \n",
       "139274 2024-01-01 02:00:00   51212   1.4  314.0   8.0  21.323 -158.149   \n",
       "139275 2024-01-01 01:00:00   51212   1.5  316.0   9.1  21.323 -158.149   \n",
       "139276 2024-01-01 01:00:00   51212   1.5  312.0   9.8  21.323 -158.149   \n",
       "139277 2024-01-01 00:00:00   51212   1.4  315.0   9.8  21.323 -158.149   \n",
       "139278 2024-01-01 00:00:00   51212   1.4  314.0  10.3  21.323 -158.149   \n",
       "\n",
       "         MWD_sin   MWD_cos  \n",
       "0       0.669131 -0.743145  \n",
       "1       0.529919 -0.848048  \n",
       "2       0.681998 -0.731354  \n",
       "3       0.615661 -0.788011  \n",
       "4       0.559193 -0.829038  \n",
       "...          ...       ...  \n",
       "139274 -0.719340  0.694658  \n",
       "139275 -0.694658  0.719340  \n",
       "139276 -0.743145  0.669131  \n",
       "139277 -0.707107  0.707107  \n",
       "139278 -0.719340  0.694658  \n",
       "\n",
       "[134924 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect('db.db')\n",
    "data = pd.read_sql_query(\"SELECT * from test\", conn)\n",
    "data = functions.cleanData(data)\n",
    "data['MWD_sin'] = np.sin(data['MWD'] * (np.pi/180))\n",
    "data['MWD_cos'] = np.cos(data['MWD'] * (np.pi/180))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6b193f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['MWD_sin'] = np.sin(data['MWD'] * (np.pi/180))\n",
    "data['MWD_cos'] = np.cos(data['MWD'] * (np.pi/180))\n",
    "data['is_present'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4105b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project lat long to x, y, with index starting at 0,0  \n",
    "# this is to fit it easily into the grid\n",
    "\n",
    "def project_lat_long_to_xy(lat, lon):\n",
    "    lat_min = np.min(lat)\n",
    "    lon_min = np.min(lon)\n",
    "    lat_max = np.max(lat)\n",
    "    lon_max = np.max(lon)\n",
    "\n",
    "    lat_range = lat_max - lat_min\n",
    "    lon_range = lon_max - lon_min\n",
    "\n",
    "    lat_normalized = (lat - lat_min) / lat_range\n",
    "    lon_normalized = (lon - lon_min) / lon_range\n",
    "\n",
    "    x = np.round(lon_normalized * (lon.shape[0] - 1)).astype(int)\n",
    "    y = np.round(lat_normalized * (lat.shape[0] - 1)).astype(int)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# data['x'], data['y'] = project_lat_long_to_xy(data['lat'], data['long'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f37d23fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_buoy_ids = data['buoy_id'].unique()\n",
    "# Get all unique times\n",
    "all_times = data['datetime'].unique()\n",
    "\n",
    "# Iterate through each time\n",
    "for time in all_times:\n",
    "    # Get the data for the current time\n",
    "    time_data = data[data['datetime'] == time]\n",
    "    \n",
    "    # Get the missing buoy_ids for the current time\n",
    "    missing_buoy_ids = np.setdiff1d(all_buoy_ids, time_data['buoy_id'])\n",
    "    \n",
    "    # if len(missing_buoy_ids) != 0: \n",
    "        # print(\"there is\", len(missing_buoy_ids), \" missing buoys at time: \", time, \"they are: \", missing_buoy_ids)  \n",
    "\n",
    "    # Iterate through each missing buoy_id\n",
    "    for buoy_id in missing_buoy_ids:\n",
    "        # Find the previous time\n",
    "        # previous_time = data[data['buoy_id'] == buoy_id]['datetime'].max()\n",
    "        \n",
    "        # Get the previous row for the missing buoy_id\n",
    "        # previous_row = data[(data['datetime'] == previous_time) & (data['buoy_id'] == buoy_id)]\n",
    "        \n",
    "        # Create a new row with the previous values and the current time\n",
    "        # new_row = previous_row.copy()\n",
    "        # new_row['datetime'] = time\n",
    "        # Create a new row with the missing buoy_id, current time, and is_present = 0\n",
    "\n",
    "        new_row = pd.DataFrame({'buoy_id': [buoy_id], 'datetime': [time], 'is_present': [0]})\n",
    "    \n",
    "        # Append the new row to the data\n",
    "        data = pd.concat([data, new_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85a58eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_bounds = [data['lat'].min(), data['lat'].max()]\n",
    "long_bounds = [data['long'].min(), data['long'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "827d2b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to normal distribution\n",
    "scaler = StandardScaler()\n",
    "data[['WVHT', 'APD', 'MWD_sin', 'MWD_cos']] = scaler.fit_transform(data[['WVHT', 'APD', 'MWD_sin', 'MWD_cos']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a5d19e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8570.89910300163"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['x'].max()\n",
    "data['y'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116f022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def project_xy_to_lat_long(x, y, lat_bounds, long_bounds):\n",
    "    lat_min, lat_max = lat_bounds\n",
    "    lon_min, lon_max = long_bounds\n",
    "\n",
    "    lat_range = lat_max - lat_min\n",
    "    lon_range = lon_max - lon_min\n",
    "\n",
    "    lat_normalized = y / (y.shape[0] - 1)\n",
    "    lon_normalized = x / (x.shape[0] - 1)\n",
    "\n",
    "    lat = lat_normalized * lat_range + lat_min\n",
    "    lon = lon_normalized * lon_range + lon_min\n",
    "\n",
    "    return lat, lon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dc267df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def mercator_projection(lat, long):\n",
    "    R = 6371  # Radius of the Earth in kilometers\n",
    "    x = R * math.radians(long)\n",
    "    y = R * math.log(math.tan(math.pi / 4 + math.radians(lat) / 2))\n",
    "    return x, y\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    data.at[index, 'x'], data.at[index, 'y'] = mercator_projection(row['lat'], row['long'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "234f55c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example: Convert your DataFrame into a 3D grid\n",
    "def create_spatial_grid(df, lat_bounds, lon_bounds, grid_size):\n",
    "    \"\"\"\n",
    "    Convert DataFrame into a spatial grid.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): Input data.\n",
    "        lat_bounds (tuple): (min_latitude, max_latitude)\n",
    "        lon_bounds (tuple): (min_longitude, max_longitude)\n",
    "        grid_size (int): The height and width of the grid.\n",
    "        \n",
    "    Returns:\n",
    "        grid (numpy array): The spatial grid of shape (grid_size, grid_size, channels).\n",
    "    \"\"\"\n",
    "    lat_steps = np.linspace(lat_bounds[0], lat_bounds[1], grid_size)\n",
    "    lon_steps = np.linspace(lon_bounds[0], lon_bounds[1], grid_size)\n",
    "    \n",
    "    # Initialize grid\n",
    "    grid = np.full((grid_size, grid_size, 4), np.nan)  # 4 channels for APD, MWD_sin, MWD_cos, WVHT\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        if row['is_present']:\n",
    "            lat_idx = np.searchsorted(lat_steps, row['lat']) - 1\n",
    "            lon_idx = np.searchsorted(lon_steps, row['long']) - 1\n",
    "            grid[lat_idx, lon_idx, :] = row[['APD', 'MWD_sin', 'MWD_cos', 'WVHT']]\n",
    "            # grid[row['x'], row['y'], :] = row[['APD', 'MWD_sin', 'MWD_cos', 'WVHT']]\n",
    "    \n",
    "    return grid\n",
    "\n",
    "# Assuming df is your DataFrame and you've defined lat_bounds, lon_bounds, and grid_size\n",
    "# grid = create_spatial_grid(data, lat_bounds, long_bounds, 50)\n",
    "\n",
    "# Normalize the data (excluding NaNs for now)\n",
    "# scaler = StandardScaler()\n",
    "# grid = np.nan_to_num(grid)  # Replace NaN with 0 (temporary)\n",
    "# flat_grid = grid.reshape(-1, 4)\n",
    "# scaler.fit(flat_grid)\n",
    "# flat_grid_scaled = scaler.transform(flat_grid)\n",
    "# grid_scaled = flat_grid_scaled.reshape(grid.shape)\n",
    "\n",
    "# Replace 0 back with NaN in normalized grid if necessary\n",
    "# grid_scaled[grid == 0] = np.nan\n",
    "\n",
    "# Handle missing data (e.g., interpolation) in grid_scaled as needed\n",
    "\n",
    "# Convert to sequences for use with ConvLSTM\n",
    "# This part depends on your sequence length and how you batch your data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae96a29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_distance_weighting(grid, power=2, max_distance=6):\n",
    "    \"\"\"\n",
    "    Impute missing values in a spatial grid using inverse distance weighting.\n",
    "    \n",
    "    Parameters:\n",
    "        grid (numpy array): The spatial grid of shape (grid_size, grid_size, channels).\n",
    "        power (int): The power parameter for the IDW calculation.\n",
    "        max_distance (int): The maximum distance to search for non-missing neighbors.\n",
    "        \n",
    "    Returns:\n",
    "        imputed_grid (numpy array): The grid with missing values imputed.\n",
    "    \"\"\"\n",
    "    imputed_grid = grid.copy()\n",
    "    grid_size = grid.shape[0]\n",
    "    \n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            if np.isnan(grid[i, j, 0]):  # Assuming if one channel is NaN, all are\n",
    "                weights = []\n",
    "                values = []\n",
    "                for ii in range(max(0, i-max_distance), min(grid_size, i+max_distance+1)):\n",
    "                    for jj in range(max(0, j-max_distance), min(grid_size, j+max_distance+1)):\n",
    "                        if not np.isnan(grid[ii, jj, 0]):  # If neighbor has data\n",
    "                            distance = np.sqrt((i-ii)**2 + (j-jj)**2)\n",
    "                            if distance == 0:  # Same location, skip to avoid division by zero\n",
    "                                continue\n",
    "                            weight = 1 / (distance ** power)\n",
    "                            weights.append(weight)\n",
    "                            values.append(grid[ii, jj, :])\n",
    "                \n",
    "                if weights:\n",
    "                    weighted_values = np.average(values, axis=0, weights=weights)\n",
    "                    imputed_grid[i, j, :] = weighted_values\n",
    "    \n",
    "    return imputed_grid\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you've already created your grids list with create_spatial_grid for each time step\n",
    "# imputed_grids = [inverse_distance_weighting(grid) for grid in grids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "841a439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = []\n",
    "times = data['datetime'].unique()\n",
    "for time in times:\n",
    "    time_data = data[data['datetime'] == time]\n",
    "    grid = create_spatial_grid(time_data, lat_bounds, long_bounds, 20)\n",
    "    grids.append(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bd19aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_grids = [inverse_distance_weighting(grid) for grid in grids]\n",
    "imputed_grids = np.stack(imputed_grids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21e3fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_grids = np.transpose(imputed_grids, (0, 3, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3839f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a2fa6b10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the 4 channels of the grids, mark the existing values with a red dot, the nans with a blue dot\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.imshow(new_grid[:,:,3])\n",
    "plt.imshow(imputed_grids[111, 2,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "251a1a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "        \n",
    "        Parameters:\n",
    "            input_dim (int) - Number of channels in input\n",
    "            hidden_dim (int) - Number of channels in hidden state\n",
    "            kernel_size (int or (int, int)) - Size of the convolutional kernel\n",
    "            bias (bool) - Whether or not to add the bias\n",
    "        \"\"\"\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # self.kernel_size = kernel_size\n",
    "        # adjustment for 2D kernel\n",
    "        if isinstance(kernel_size, int):\n",
    "            self.kernel_size = (kernel_size, kernel_size)\n",
    "        else:\n",
    "            self.kernel_size = kernel_size\n",
    "        self.padding = (self.kernel_size[0] // 2, self.kernel_size[1] // 2)  # Adjusted for tuple input\n",
    "        self.bias = bias\n",
    "\n",
    "        # self.padding = kernel_size // 2\n",
    "        # self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,  # for input_gate, forget_gate, current_state, output_gate\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel dimension\n",
    "\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        self.layers = nn.ModuleList([ConvLSTMCell(self.input_dim if i == 0 else self.hidden_dim,\n",
    "                                                  self.hidden_dim, self.kernel_size, self.bias)\n",
    "                                     for i in range(self.num_layers)])\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            input_tensor: 5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
    "            hidden_state: None or a list of tuples of hidden state representations\n",
    "        Returns:\n",
    "            last_state_list, layer_output\n",
    "        \"\"\"\n",
    "        if not hidden_state:\n",
    "            hidden_state = self._init_hidden(batch_size=input_tensor.size(0),\n",
    "                                             image_size=(input_tensor.size(3), input_tensor.size(4)))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.layers[layer_idx](cur_layer_input[:, t, :, :, :], (h, c))\n",
    "                output_inner.append(h)\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append((h, c))\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "\n",
    "        return layer_output_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f955539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        self.layers = nn.ModuleList([ConvLSTMCell(self.input_dim if i == 0 else self.hidden_dim,\n",
    "                                                  self.hidden_dim, self.kernel_size, self.bias)\n",
    "                                     for i in range(self.num_layers)])\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            input_tensor: 5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
    "            hidden_state: None or a list of tuples of hidden state representations\n",
    "        Returns:\n",
    "            last_state_list, layer_output\n",
    "        \"\"\"\n",
    "        if not hidden_state:\n",
    "            # Assumes hidden_state is None, then initialize it using a new method\n",
    "            hidden_state = self._init_hidden(batch_size=input_tensor.size(0),\n",
    "                                             image_size=(input_tensor.size(3), input_tensor.size(4)))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.layers[layer_idx](cur_layer_input[:, t, :, :, :], (h, c))\n",
    "                output_inner.append(h)\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append((h, c))\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.layers[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "662595ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import optim\n",
    "\n",
    "class SpatialTemporalDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length=5):\n",
    "        self.data = data.astype(np.float32)\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0] - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # the sequences should always be of length 8\n",
    "        input_sequence = self.data[idx:idx+self.sequence_length]\n",
    "        target_sequence = self.data[idx+1:idx+self.sequence_length+1]\n",
    "\n",
    "        # if input_sequence.shape[0] < self.sequence_length:\n",
    "        #     # pad the sequence with the last available data\n",
    "        #     padding = np.tile(input_sequence[-1], (self.sequence_length - input_sequence.shape[0], 1))\n",
    "        #     input_sequence = np.concatenate([input_sequence, padding], axis=0)\n",
    "        #     target_sequence = np.concatenate([target_sequence, padding], axis=0)\n",
    "\n",
    "        # return (self.data[idx:idx+self.sequence_length], self.data[idx+1:idx+self.sequence_length+1])\n",
    "        return (input_sequence, target_sequence)\n",
    "\n",
    "# Prepare the dataset\n",
    "X = np.array(imputed_grids)  # Assuming 'grids' is your np.array of shape (1104, 20, 20, 4)\n",
    "\n",
    "# Split the data into training and testing\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SpatialTemporalDataset(X_train)\n",
    "test_dataset = SpatialTemporalDataset(X_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab9ce5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ConvLSTM(input_dim=4, hidden_dim=4, kernel_size=(3, 3), num_layers=2).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8147e3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "1 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "2 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "3 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "4 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "5 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "6 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "7 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "8 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "9 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "10 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "11 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "12 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "13 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "14 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "15 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "16 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "17 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "18 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "19 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "20 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "21 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "22 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "23 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "24 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "25 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "26 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "27 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "28 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "29 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "30 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "31 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "32 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "33 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "34 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "35 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "36 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "37 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "38 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "39 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "40 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "41 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "42 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "43 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "44 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "45 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "46 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "47 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "48 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "49 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "50 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "51 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "52 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "53 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "54 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "55 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "56 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "57 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "58 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "59 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "60 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "61 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "62 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "63 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "64 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "65 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "66 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "67 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "68 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "69 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "70 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "71 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "72 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "73 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "74 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "75 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "76 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "77 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "78 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "79 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "80 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "81 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "82 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "83 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "84 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "85 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "86 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "87 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "88 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "89 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "90 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "91 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "92 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "93 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "94 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "95 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "96 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "97 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "98 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "99 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "100 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "101 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "102 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "103 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "104 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "105 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "106 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "107 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "108 torch.Size([8, 5, 4, 20, 20]) <class 'torch.Tensor'>\n",
      "109 torch.Size([6, 5, 4, 20, 20]) <class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn[35], line 36\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     34\u001b[0m         inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     35\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m---> 36\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     37\u001b[0m         total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     39\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_loader)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mmse_loss(\u001b[38;5;28minput\u001b[39m, target, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3318\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, target):\n\u001b[1;32m   3315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   3316\u001b[0m         mse_loss, (\u001b[38;5;28minput\u001b[39m, target), \u001b[38;5;28minput\u001b[39m, target, size_average\u001b[38;5;241m=\u001b[39msize_average, reduce\u001b[38;5;241m=\u001b[39mreduce, reduction\u001b[38;5;241m=\u001b[39mreduction\n\u001b[1;32m   3317\u001b[0m     )\n\u001b[0;32m-> 3318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[1;32m   3319\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   3320\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis will likely lead to incorrect results due to broadcasting. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3322\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3323\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   3324\u001b[0m     )\n\u001b[1;32m   3325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        i = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(inputs)\n",
    "            # outputs = torch.stack(outputs)\n",
    "            # outputs = outputs.squeeze()\n",
    "            outputs = outputs[0]\n",
    "\n",
    "            print(i, outputs.shape, type(outputs))  # Check the type of the output\n",
    "            i+=1\n",
    "            if isinstance(outputs, tuple):\n",
    "                print(\"Output is a tuple. Length:\", len(outputs))\n",
    "                predictions = outputs[0]  # Assuming the first element is always the prediction\n",
    "            else:\n",
    "                predictions = outputs\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            # print(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "        test_loss = total_loss / len(test_loader)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742b2aad",
   "metadata": {},
   "source": [
    "### simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d0de5f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleConvLSTM, self).__init__()\n",
    "        self.conv_lstm = nn.LSTM(input_size=400, hidden_size=100, num_layers=1, batch_first=True)\n",
    "        self.decoder = nn.Linear(100, 400)  # Decodes back to grid size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assuming x is of shape (batch, seq_len, channels, height, width)\n",
    "        # Flatten spatial dimensions\n",
    "        batch_size, seq_len, channels, height, width = x.size()\n",
    "        x = x.view(batch_size, seq_len, channels*height*width)\n",
    "        \n",
    "        # ConvLSTM\n",
    "        lstm_out, _ = self.conv_lstm(x)\n",
    "        \n",
    "        # Decode the output\n",
    "        decoded = self.decoder(lstm_out)\n",
    "        \n",
    "        # Reshape to original grid shape (for the last 72 outputs)\n",
    "        decoded = decoded[:, -72:].view(batch_size, 72, channels, height, width)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f106403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class SpatialDataset(Dataset):\n",
    "    def __init__(self, data, input_seq_length=96, output_seq_length=72):\n",
    "        \"\"\"\n",
    "        data: The full dataset of shape (total_time_steps, channels, height, width).\n",
    "        input_seq_length: The length of the input sequences.\n",
    "        output_seq_length: The length of the output sequences.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.input_seq_length = input_seq_length\n",
    "        self.output_seq_length = output_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        # Ensure there is enough data for the output sequence\n",
    "        return len(self.data) - self.input_seq_length - self.output_seq_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # idx is the start index of the input sequence\n",
    "        input_start = idx\n",
    "        input_end = idx + self.input_seq_length\n",
    "        output_end = input_end + self.output_seq_length\n",
    "        input_sequence = self.data[input_start:input_end]\n",
    "        output_sequence = self.data[input_end:output_end]\n",
    "        return torch.tensor(input_sequence, dtype=torch.float), torch.tensor(output_sequence, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6fd038bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "937"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  train_size + valid_size\n",
    "len(SpatialDataset(full_data_tensor))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2265d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_tensor = torch.tensor(imputed_grids, dtype=torch.float)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "# train_size = int(0.8 * len(full_data_tensor))\n",
    "# valid_size = len(full_data_tensor) - train_size\n",
    "\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(SpatialDataset(full_data_tensor), [800, 137])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "df9f8dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in valid_loader:\n",
    "                outputs = model(inputs)\n",
    "                val_loss = criterion(outputs, targets)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Validation Loss: {total_val_loss / len(valid_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2686a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleConvLSTM().float()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ceca2fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7j/ld2slh_j2c9_ldzcsddyv7hh0000gn/T/ipykernel_70946/1048197873.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(input_sequence, dtype=torch.float), torch.tensor(output_sequence, dtype=torch.float)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Adjust as needed\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs)\n",
      "Cell \u001b[0;32mIn[49], line 7\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, valid_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      6\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 7\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m      8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m      9\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[37], line 11\u001b[0m, in \u001b[0;36mSimpleConvLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Assuming x is of shape (batch, seq_len, channels, height, width)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Flatten spatial dimensions\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     batch_size, seq_len, channels, height, width \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size, seq_len, channels\u001b[38;5;241m*\u001b[39mheight\u001b[38;5;241m*\u001b[39mwidth)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# ConvLSTM\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     lstm_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_lstm(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "num_epochs = 10  # Adjust as needed\n",
    "train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc846d10",
   "metadata": {},
   "source": [
    "### Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f0fd651c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_moving_average(grid):\n",
    "    \"\"\"\n",
    "    Calculate a simple spatial moving average for a grid, averaging each cell with its immediate neighbors.\n",
    "\n",
    "    :param grid: Numpy array of shape (channels, height, width)\n",
    "    :return: Spatially smoothed grid\n",
    "    \"\"\"\n",
    "    smoothed_grid = np.copy(grid)\n",
    "    channels, height, width = grid.shape\n",
    "    for h in range(1, height - 1):\n",
    "        for w in range(1, width - 1):\n",
    "            for c in range(channels):\n",
    "                smoothed_grid[c, h, w] = np.mean(grid[c, h-1:h+2, w-1:w+2])\n",
    "    return smoothed_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d5347bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def temporal_moving_average(data, window_size):\n",
    "    \"\"\"\n",
    "    Calculate the moving average over the last `window_size` time steps for each spatial position.\n",
    "\n",
    "    :param data: Numpy array of shape (time_steps, channels, height, width)\n",
    "    :param window_size: Number of time steps to include in the moving average\n",
    "    :return: Moving average for the last time step\n",
    "    \"\"\"\n",
    "    moving_avg = np.mean(data[-window_size:], axis=0)\n",
    "    return moving_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "90ed22dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_polynomial_to_temporal_data(data, degree=2):\n",
    "    \"\"\"\n",
    "    Fit a polynomial curve of a given degree to the temporal data of each spatial cell and channel.\n",
    "\n",
    "    :param data: Numpy array of shape (time_steps, channels, height, width).\n",
    "    :param degree: Degree of the polynomial to fit.\n",
    "    :return: Coefficients of the fitted polynomial for each spatial cell and channel.\n",
    "    \"\"\"\n",
    "    time_steps, channels, height, width = data.shape\n",
    "    # Time indices for polynomial fitting\n",
    "    time_indices = np.arange(time_steps)\n",
    "    \n",
    "    # Initialize an array to hold the polynomial coefficients\n",
    "    # Shape: (channels, height, width, degree + 1)\n",
    "    poly_coeffs = np.zeros((channels, height, width, degree + 1))\n",
    "    \n",
    "    for c in range(channels):\n",
    "        for h in range(height):\n",
    "            for w in range(width):\n",
    "                # Fit a polynomial for each spatial cell and channel\n",
    "                coeffs = np.polyfit(time_indices, data[:, c, h, w], degree)\n",
    "                poly_coeffs[c, h, w, :] = coeffs\n",
    "                \n",
    "    return poly_coeffs\n",
    "\n",
    "def predict_with_polynomial(coeffs, num_future_steps, start_time):\n",
    "    \"\"\"\n",
    "    Predict future steps using the polynomial coefficients obtained from fitting.\n",
    "\n",
    "    :param coeffs: Polynomial coefficients from fitting, shape (channels, height, width, degree + 1).\n",
    "    :param num_future_steps: Number of future time steps to predict.\n",
    "    :param start_time: The starting time index for prediction.\n",
    "    :return: Predicted values for the future time steps, shape (num_future_steps, channels, height, width).\n",
    "    \"\"\"\n",
    "    channels, height, width, _ = coeffs.shape\n",
    "    future_indices = np.arange(start_time, start_time + num_future_steps)\n",
    "    predictions = np.zeros((num_future_steps, channels, height, width))\n",
    "    \n",
    "    for c in range(channels):\n",
    "        for h in range(height):\n",
    "            for w in range(width):\n",
    "                # Predict future steps using the polynomial coefficients\n",
    "                for i, t in enumerate(future_indices):\n",
    "                    predictions[i, c, h, w] = np.polyval(coeffs[c, h, w, :], t)\n",
    "                    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5455a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 2  # Degree of polynomial to fit\n",
    "poly_coeffs = fit_polynomial_to_temporal_data(imputed_grids, degree=degree)\n",
    "num_future_steps = 72\n",
    "start_time = data.shape[0]  # Assuming prediction starts right after the last time step in the data\n",
    "predictions = predict_with_polynomial(poly_coeffs, num_future_steps, start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5bf6b134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_future_steps(data, num_predictions=72, temporal_window=96):\n",
    "    predictions = []\n",
    "    current_data = np.copy(data)\n",
    "    for _ in range(num_predictions):\n",
    "        # Calculate temporal moving average\n",
    "        temp_avg = temporal_moving_average(current_data, temporal_window)\n",
    "        # Apply spatial moving average\n",
    "        spatial_avg = spatial_moving_average(temp_avg)\n",
    "        predictions.append(spatial_avg)\n",
    "        # Append the prediction to current_data for subsequent predictions\n",
    "        current_data = np.concatenate((current_data, spatial_avg[np.newaxis, :]), axis=0)\n",
    "    return predictions\n",
    "otherpredictions = predict_future_steps(imputed_grids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "010bf7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn list into np array\n",
    "otherpredictions = np.array(otherpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "daaebafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1441b7097b34f22babaf0f9040c7947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='time_step', max=71), Output()), _dom_classes=('widget-inâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_predictions(time_step)>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "# I want to have a scrollable way to see the predictions over time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predictions(time_step):\n",
    "    plt.imshow(otherpredictions[time_step, 2, :, :])\n",
    "    plt.title(f'Time Step: {time_step}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "widgets.interact(plot_predictions, time_step=widgets.IntSlider(min=0, max=71, step=1, value=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9460b057",
   "metadata": {},
   "source": [
    "### Existing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7f5321e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# The URL of the GRIB2 file\n",
    "url = \"https://nomads.ncep.noaa.gov/pub/data/nccf/com/gfs/prod/gdas.20240218/18/wave/gridded/gdaswave.t18z.arctic.9km.f000.grib2\"\n",
    "\n",
    "# The path where you want to save the downloaded file\n",
    "file_path = \"gdaswave.t18z.arctic.9km.f000.grib2\"\n",
    "\n",
    "# Download the file\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # This will raise an exception if there was an error downloading the file\n",
    "\n",
    "# Write the file's content to a file\n",
    "with open(file_path, \"wb\") as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "print(\"File downloaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d65a260",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unrecognized engine cfgrib must be one of: ['netcdf4', 'scipy', 'store']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgdaswave.t18z.arctic.9km.f000.grib2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Open the GRIB2 file\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m ds \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(file_path, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcfgrib\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Print an overview of the data\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(ds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/api.py:552\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_array_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    550\u001b[0m     from_array_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 552\u001b[0m backend \u001b[38;5;241m=\u001b[39m plugins\u001b[38;5;241m.\u001b[39mget_backend(engine)\n\u001b[1;32m    554\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    555\u001b[0m     decode_cf,\n\u001b[1;32m    556\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    563\u001b[0m )\n\u001b[1;32m    565\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/plugins.py:205\u001b[0m, in \u001b[0;36mget_backend\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m    203\u001b[0m     engines \u001b[38;5;241m=\u001b[39m list_engines()\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m engines:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    206\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munrecognized engine \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be one of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(engines)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    207\u001b[0m         )\n\u001b[1;32m    208\u001b[0m     backend \u001b[38;5;241m=\u001b[39m engines[engine]\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(engine, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(engine, BackendEntrypoint):\n",
      "\u001b[0;31mValueError\u001b[0m: unrecognized engine cfgrib must be one of: ['netcdf4', 'scipy', 'store']"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Path to the downloaded GRIB2 file\n",
    "file_path = \"gdaswave.t18z.arctic.9km.f000.grib2\"\n",
    "\n",
    "# Open the GRIB2 file\n",
    "ds = xr.open_dataset(file_path, engine=\"cfgrib\")\n",
    "\n",
    "# Print an overview of the data\n",
    "print(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528770eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
